# -*- coding: utf-8 -*-
"""StrawBerry.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N-TVJM9ipAm__MOY8UYJcOzOdSPejZys
"""

#!pip install giotto-tda

import numpy as np
from scipy.io import arff
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from gtda.time_series import TakensEmbedding
from gtda.homology import VietorisRipsPersistence
from gtda.diagrams import BettiCurve
from matplotlib import pyplot as plt
from matplotlib.pyplot import figure
import tensorflow as tf
from tensorflow import keras
from sklearn.utils import class_weight
from numpy import expand_dims
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report
from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials
import os
from sklearn.utils import shuffle

def load_data(path_train, path_test):
  """ Read data and labels from arff file and return X train X test and labels y,
      in shape (n_samples, length, 1) and (n_samples)

      Parameters
      ----------
      path_train: string path of the train set
      path:test: string path of the test set

      Returns
      --------
      X_train: np array of shape (n_samples_train, ts_length) ts:= time series
      X_test: np.array of sahpe  (n_samples_test, ts_length)
      y_train: np. array of shape (n_samples_train,)
      y_test: np.array of shape (n_samples_test,)
  """
  #Train data
  data_train = arff.loadarff(path_train)
  df_train = (pd.DataFrame(data_train[0])).to_numpy()
  X_train = [df_train[i,:-1] for i in range(len(df_train))]
  y_train = [int(float(df_train[i,-1].decode('utf-8'))) - 1 for i in range(len(df_train[:,-1]))]
  #Test data
  data_test = arff.loadarff(path_test)
  df_test = (pd.DataFrame(data_test[0])).to_numpy()
  X_test = [df_test[i,:-1] for i in range(len(df_test))]
  y_test = [int(float(df_test[i,-1].decode('utf-8'))) - 1 for i in range(len(df_test[:,-1]))]

  return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)

def get_betti_curv(X_train, X_test, n_bins=200, max_edge_length=1):
  """Create the point Cloud from the time series using Takens' Embedding with parameters
     dimension=3, delay=1 and it calculates the homology of this point clouds to create the
     betti curves and then we create betti sequences

     Parameters
     ----------
     X_train: np array of shape (n_samples_train, ts_length)
     X_test: np.array of sahpe  (n_samples_test, ts_length)
     n_bins: number of points for a discretization of Betti Curves
     max_edge_length: Max length considered in Vietoris Rips complexes

     Returns
     --------
     BC_train: Betti curves of train data in shape (n_samples_train, 2, n_bins)
     BC_test: Betti curves of test data in shape (n_samples_test, 2, n_bins)
     Hom_train: Birth and death points of persistence diagram in train data
     Hom_test: Birth and death points of persistence diagrama in test data
     BC: Betti Curve Class fitted for visualization purposes
  """
  #Delay embedding in order to create cloud point in 3D
  TE = TakensEmbedding(time_delay=1, dimension=3)
  Embd_train = TE.fit_transform(X_train)
  Embd_test = TE.fit_transform(X_test)

  # Birth and death classes
  VR = VietorisRipsPersistence(max_edge_length=max_edge_length) # default homology_dimensions=(0,1)
  Hom_train = VR.fit_transform(Embd_train)
  Hom_test = VR.fit_transform(Embd_test)

  #Calculate Betti Curve
  BC = BettiCurve(n_bins = n_bins)
  BC_train = BC.fit_transform(Hom_train)
  BC_test = BC.fit_transform(Hom_test)
  return BC_train, BC_test, Hom_train, Hom_test, BC

def get_betti_seq(BC_train, BC_test, standarized = True):
  """Given the Betti Curves it creates the betti sequences with discretization mesh (n_bins) given
  function get_betti_curv

  Parameters
  -----------
  BC_train: np array of Betti Curves of train data set in shape  (n_samples_train, 2, n_bins)
  BC_test: Betti Curves of test data set in shape (n_samples_test, 2, n_bins)
  standarized: whether the betti sequence are going to be standarized or not

  Returns
  -------
  BS_train: np array Betti Sequences of train data set in shape (n_samples_train, 2 * n_bins, 1)
  BS_test: np array Betti Sequences of test data set in shape (n_samples_test, 2 * n_bins, 1)
  """
  BS_train = [np.concatenate([BC_train[i,0], BC_train[i,1]]) for i in range(len(BC_train))]
  BS_test = [np.concatenate([BC_test[i,0], BC_test[i,1]]) for i in range(len(BC_test))]
  if(standarized):
    BS_train = [(BS_train[i] - np.mean(BS_train[i])) / np.std(BS_train[i]) for i in range(len(BS_train))]
    BS_test = [(BS_test[i] - np.mean(BS_test[i])) / np.std(BS_test[i]) for i in range(len(BS_test))]
  return np.expand_dims(BS_train, axis=-1), np.expand_dims(BS_test, axis=-1)

def load_full(path_train, path_test, path_folder_save, n_bins=200, max_edge_length=1, standarized=True):
  """Using Only paths to the data, path to the folder where the data will be saved (should be empty if
  betti sequences has not been calculated) creates betti sequences ready to enter to a neural network

  Parameters
  ----------
  path_train: a string containing the path to the train data in arff extension
  path_test: a string containing the path to the test data in arff extension
  path_folder_save: a string cointaning the path to the folder where the data
  sould be written after being calculated, it should be empty if the betti
  seuqneces has not been calculated, if they have already been calculated it
  should have the data in txt formata
  n_bins: number of points for a discretization of Betti Curves
  max_edge_length: Max length considered in Vietoris Rips complexes
  standarized: whether the betti sequence are going to be standarized or not

  Returns
  --------
  BS_train: np array Betti Sequences of train data set in shape (n_samples_train, 2 * n_bins, 1)
  BS_test: np array Betti Sequences of test data set in shape (n_samples_test, 2 * n_bins, 1)
  y_train: np. array of shape (n_samples_train,)
  y_test: np.array of shape (n_samples_test,)
  Hom_train: Birth and death points of persistence diagram in train data
  Hom_test: Birth and death points of persistence diagrama in test data
  """
  if len(os.listdir(path_folder_save)) == 0: #Check if the data has already been calculated
    X_train, X_test, y_train, y_test = load_data(path_train, path_test)
    BC_train, BC_test, Hom_train, Hom_test, BC = get_betti_curv(X_train, X_test, n_bins=n_bins, max_edge_length=max_edge_length)
    BS_train, BS_test = get_betti_seq(BC_train, BC_test, standarized=standarized)
    to_save =[BS_train, BS_test, Hom_train, Hom_test, np.expand_dims(y_train,axis=-1), np.expand_dims(y_test,axis=-1)]
    paths = ['BS_train', 'BS_test', 'Hom_train', 'Hom_test', 'y_train', 'y_test']
    for i in range(len(paths)):
      file = open(path_folder_save + '/' + paths[i] + '.txt', "w")
      for row in to_save[i]:
        np.savetxt(file, row)
      file.close()
  else:
    #Load the precalculated data
    BS_train = np.loadtxt(path_folder_save + '/BS_train.txt')
    BS_test = np.loadtxt(path_folder_save + '/BS_test.txt')
    Hom_train = np.loadtxt(path_folder_save + '/Hom_train.txt')
    Hom_test = np.loadtxt(path_folder_save + '/Hom_test.txt')
    y_train = np.loadtxt(path_folder_save + '/y_train.txt')
    y_test = np.loadtxt(path_folder_save + '/y_test.txt')
    #reshape data
    samples_train = int(len(BS_train) / (2 * n_bins))
    samples_test = int(len(BS_test) / (2 * n_bins))
    BS_train = BS_train.reshape(samples_train, int(2 * n_bins), 1)
    BS_test = BS_test.reshape(samples_test, int(2 * n_bins), 1)
    Hom_train = Hom_train.reshape(samples_train,int(len(Hom_train) / (samples_train)), 3)
    Hom_test = Hom_test.reshape(samples_test,int(len(Hom_test) / (samples_test)), 3)
  return BS_train, BS_test, y_train, y_test, Hom_train, Hom_test



#### Problema que cuando son multivariados se repite todos


def load_full_multivariate(path_train, path_test, path_folder_save, n_dims, n_bins=200, max_edge_length=1, standarized=True):
  """Load all the data when there are multivariate time series, it also converts its in individuals betti sequences.

  Parameters
  ----------
  path_train: an array of strings containing the paths to the train data in arff extension (in each dimension)
  path_test: an array of strings containing the path to the test data in arff extension (in each dimension)
  path_folder_save: a string cointaning the path to the folder where the data
  sould be written after being calculated, it should be empty if the betti
  seuqneces has not been calculated, if they have already been calculated it
  should have the data in txt formata
  n_bins: number of points for a discretization of Betti Curves
  max_edge_length: Max length considered in Vietoris Rips complexes
  standarized: whether the betti sequence are going to be standarized or not

  Returns
  --------
  BS_train: np array Betti Sequences of train data set in shape (n_dims, n_samples_train, 2 * n_bins, 1)
  BS_test: np array Betti Sequences of test data set in shape (n_dims, n_samples_test, 2 * n_bins, 1)
  y_train: np. array of shape (n_samples_train,)
  y_test: np.array of shape (n_samples_test,)
  Hom_train: Birth and death points of persistence diagram in train data
  Hom_test: Birth and death points of persistence diagrama in test data
  """

  BS_TrD = [None for i in range(n_dims)]
  BS_TeD = [None for i in range(n_dims)]
  Hom_trD = [None for i in range(n_dims)]
  Hom_teD = [None for i in range(n_dims)]
  if len(os.listdir(path_folder_save)) == 0: #Check if the data has already been calculated
    for i in range(n_dims):
      X_train, X_test, y_train, y_test = load_data(path_train[n_dims], path_test[n_dims])
      BC_train, BC_test, Hom_train, Hom_test, BC = get_betti_curv(X_train, X_test, n_bins=n_bins, max_edge_length=max_edge_length)
      BS_train, BS_test = get_betti_seq(BC_train, BC_test, standarized=standarized)
      to_save =[BS_train, BS_test, Hom_train, Hom_test, np.expand_dims(y_train,axis=-1), np.expand_dims(y_test,axis=-1)]
      paths = ['BS_train', 'BS_test', 'Hom_train', 'Hom_test', 'y_train', 'y_test']
      folder_save_dim = "Betti_curve_dim" + str(i)
      new_folder_save = path_folder_save + "/" + folder_save_dim
      os.mkdir(new_folder_save)
      for i in range(len(paths)):
        file = open(new_folder_save + '/' + paths[i] + '.txt', "w")
        for row in to_save[i]:
          np.savetxt(file, row)
        file.close()
      BS_TrD[i] = BS_train
      BS_TeD[i] = BS_test
      Hom_trD[i] = Hom_train
      Hom_teD[i] = Hom_test
  else:
    for i in range(n_dims):
      folder_save_dim = "Betti_curve_dim" + str(i)
      new_folder_save = path_folder_save + "/" + folder_save_dim
      BS_train = np.loadtxt(new_folder_save + '/BS_train.txt')
      BS_test = np.loadtxt(new_folder_save + '/BS_test.txt')
      Hom_train = np.loadtxt(new_folder_save + '/Hom_train.txt')
      Hom_test = np.loadtxt(new_folder_save + '/Hom_test.txt')
      y_train = np.loadtxt(new_folder_save + '/y_train.txt')
      y_test = np.loadtxt(new_folder_save + '/y_test.txt')
      #reshape data
      samples_train = int(len(BS_train) / (2 * n_bins))
      samples_test = int(len(BS_test) / (2 * n_bins))
      BS_train = BS_train.reshape(samples_train, int(2 * n_bins), 1)
      BS_test = BS_test.reshape(samples_test, int(2 * n_bins), 1)
      Hom_train = Hom_train.reshape(samples_train,int(len(Hom_train) / (samples_train)), 3)
      Hom_test = Hom_test.reshape(samples_test,int(len(Hom_test) / (samples_test)), 3)
      BS_TrD[i] = BS_train
      BS_TeD[i] = BS_test
      Hom_trD[i] = Hom_train
      Hom_teD[i] = Hom_test
  return BS_trD, BS_teD, y_train, y_test, Hom_trD, Hom_teD

"""# ML part"""

def visualize_data(X_train=[], X_test=[], Hom_train=[], Hom_test=[], data='train', n_bins=200, sample=0):
  BC = BettiCurve(n_bins=n_bins)
  if(data =='train' or data == 'train+test'):
      fig = go.Figure()
      fig.add_trace(go.Scatter(x=list(range(len(X_train[sample]))), y=X_train[sample])) # fill down to xaxis
      fig.update_layout(title = "Train")
      fig.show()
      BC_train = BC.fit_transform(Hom_train)
      BC.plot(BC_train, sample=sample).show()
  if(data == 'test'or data == 'train+test'):
      fig_test = go.Figure()
      fig_test.add_trace(go.Scatter(x=list(range(len(X_train[sample]))), y=X_train[sample])) # fill down to xaxis
      fig_test.update_layout(title = "Test")
      fig_test.show()
      BC_test = BC.fit_transform(Hom_test)
      BC.plot(BC_test, sample=sample).show()
  pass

def vis_classes(X_train=[], X_test=[], y_train=[], y_test=[], index=0):
  """Visualize time series and betti sequences one per class
  Parameters
  ----------
  X_train: np array of shape (n_samples_train, ts_length) ts:= time series
  X_test: np.array of sahpe  (n_samples_test, ts_length)
  y_train: np. array of shape (n_samples_train,)
  y_test: np.array of shape (n_samples_test,)
  index: index where to start to search
  """
  fig_train = go.Figure()
  fig_test = go.Figure()
  X_train, y_train = shuffle(X_train,y_train)
  X_test, y_test = shuffle(X_test,y_test)
  for idx in np.unique(y_train):
    for i in range(index,len(y_train)):
      if(y_train[i] == idx):
        fig_train.add_trace(go.Scatter(x=list(range(len(X_train[i]))), y=X_train[i]))
        break
  fig_train.update_layout(title="Train")
  for idx in np.unique(y_test):
    for i in range(index,len(X_test)):
      if(y_test[i] == idx):
        fig_test.add_trace(go.Scatter(x=list(range(len(X_test[i]))), y=X_test[i]))
        break
  fig_test.update_layout(title="Test")
  fig_train.show()
  fig_test.show()

def vis_class(X_train=[], X_test=[], y_train=[], y_test=[], quant=5, clase=0):
  """Visualize certain quantity of times time series of class clase
  Parameters
  ---------
  X_train: np array of shape (n_samples_train, ts_length) ts:= time series
  X_test: np.array of sahpe  (n_samples_test, ts_length)
  y_train: np. array of shape (n_samples_train,)
  y_test: np.array of shape (n_samples_test,)
  quant: quantity of time series of class clase to plot
  clase: class of time series to plot
  """
  fig_train = go.Figure()
  fig_test = go.Figure()
  X_train, y_train = shuffle(X_train,y_train)
  X_test, y_test = shuffle(X_test,y_test)
  limit_train=0
  for i in range(len(y_train)):
    if(y_train[i] == clase and limit_train < quant):
      fig_train.add_trace(go.Scatter(x=list(range(len(X_train[i]))), y=X_train[i]))
      limit_train += 1
  limit_test=0
  for i in range(len(y_test)):
    if(y_test[i] == clase and limit_test < quant):
      fig_test.add_trace(go.Scatter(x=list(range(len(X_test[i]))), y=X_test[i]))
      limit_test += 1
  fig_test.update_layout(title="Test")
  fig_train.show()
  fig_test.show()

def train(model, BS_train, y_train, parameters):
  """Given the model ->unfitted<- we train the model with betti sequences BS_train, y_train
  and parameters needed for training

  Parameters
  ------------
  model: Keras model that has already been compiled but not fitted
  BS_ train: np array Betti Sequences of train data set in shape (n_samples_train, 2 * n_bins, 1)
  y_train: np. array of shape (n_samples_train,)
  parameters: array with hyperparamets for trainning (more detail in firsts lines of code)

  Returns
  ------------
  history: DataFrame that is given when we fit the model
  model: keras model fitted
  """
  BS_train, y_train = shuffle(BS_train, y_train)
  epochs = parameters[0]
  validation_split = parameters[1]
  verbose = parameters[2]
  ClassWeight = parameters[3]
  batch_size = parameters[4]
  if ClassWeight:
    weight = class_weight.compute_class_weight('balanced',classes=np.unique(y_train), y=y_train)
    class_weights = dict(zip(range(len(weight)),weight))
    history = model.fit(BS_train, to_categorical(y_train), epochs=epochs, validation_split=validation_split,
                        verbose = verbose, class_weight=class_weights, batch_size=batch_size)
  else:
    history = model.fit(BS_train, to_categorical(y_train), epochs=epochs, validation_split=validation_split,
                        verbose = verbose, batch_size=batch_size)
  return history, model

def acc_avg(iterations, model, BS_train, y_train, BS_test, y_test, parameters):
  """Model evaluation model should be unffited
  Parameters
  ------------
  iterations: numbers of samples to calculate
  BS_train: np array Betti Sequences of train data set in shape (n_samples_train, 2 * n_bins, 1)
  BS_test: np array Betti Sequences of test data set in shape (n_samples_test, 2 * n_bins, 1)
  y_train: np. array of shape (n_samples_train,)
  y_test: np.array of shape (n_samples_test,)
  parameters: array with hyperparamets for trainning

  Returns
  --------
  mean: mean of the test set accuracy sample
  std: standard deviation of test set accuracy sample

  """
  BS_test, y_test = shuffle(BS_test, y_test)
  avg = []
  for i in range(iterations):
    model_fitted, history = train(model, BS_train, y_train, parameters)
    loss_test, acc_test = model.evaluate(BS_test, to_categorical(y_test))
    avg.append(acc_test)
  return np.mean(avg), np.std(avg)

def plot_history(history):
  """Plot history of loss, loss_acuracy, val_accuracy and accuracy when training
  Parameters
  ----------
  history: Dataframe as given by keras after fitting model
  """
  pd.DataFrame(history.history).plot(figsize=(16,10))
  plt.grid(True)
  plt.gca().set_ylim(0.2,2)
  plt.show()
  pass

def full_report(model,BS_train, y_train, BS_test, y_test, parameters):
  """A full report about a keras model compiled but not already fitted, it contains model summary, classification report,
  test set accuracy and confusion matrix

  Parameters
  ----------
  model: Keras model that has already been compiled but not fitted
  BS_train: np array Betti Sequences of train data set in shape (n_samples_train, 2 * n_bins, 1)
  BS_test: np array Betti Sequences of test data set in shape (n_samples_test, 2 * n_bins, 1)
  y_train: np. array of shape (n_samples_train,)
  y_test: np.array of shape (n_samples_test,)
  parameters: array with hyperparamets for trainning
  """

  history, model = train(model, BS_train, y_train, parameters)
  print(model.summary())
  plot_history(history)
  y_pred = model.predict(BS_test)
  print(classification_report(y_test, y_pred.argmax(axis=1)))
  loss_test, acc_test = model.evaluate(BS_test, to_categorical(y_test))
  print('Test set accuracy:', acc_test)
  pass

-
